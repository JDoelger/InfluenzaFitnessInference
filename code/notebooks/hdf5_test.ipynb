{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "repository_path='C:/Users/julia/Documents/Resources/InfluenzaFitnessLandscape/NewApproachFromMarch2021/InfluenzaFitnessInference/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.random.random(size=(1000,1000))\n",
    "matrix2 = np.random.random(size=(10000,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create and write to hdf5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(repository_path + 'results/test_data.h5','w') as hdf:\n",
    "    hdf.create_dataset('dataset1',data=matrix1)\n",
    "    hdf.create_dataset('dataset2',data=matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of datasets in this file: \n",
      " ['dataset1', 'dataset2']\n",
      "Shape of dataset1: \n",
      " (1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(repository_path + 'results/test_data.h5','r') as hdf:\n",
    "    ls = list(hdf.keys()) \n",
    "    print('List of datasets in this file: \\n',ls)\n",
    "    data = hdf.get('dataset1')\n",
    "    dataset1 = np.array(data)\n",
    "    print('Shape of dataset1: \\n',dataset1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create groups in hdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.random.random(size=(1000,1000))\n",
    "matrix2 = np.random.random(size=(1000,1000))\n",
    "matrix3 = np.random.random(size=(1000,1000))\n",
    "matrix4 = np.random.random(size=(1000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(repository_path + 'results/test_groups.h5', 'w') as hdf:\n",
    "    G1 = hdf.create_group('Group1')\n",
    "    G1.create_dataset('dataset1',data = matrix1)\n",
    "    G1.create_dataset('dataset4',data = matrix4)\n",
    "    \n",
    "    G21 = hdf.create_group('Group2/SubGroup1')\n",
    "    G21.create_dataset('dataset3',data = matrix3)\n",
    "    \n",
    "    G22 = hdf.create_group('Group2/SubGroup2')\n",
    "    G22.create_dataset('dataset2',data = matrix2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read groups in hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in the base directory: [('Group1', <HDF5 group \"/Group1\" (2 members)>), ('Group2', <HDF5 group \"/Group2\" (2 members)>)]\n",
      "Items in Group2: [('SubGroup1', <HDF5 group \"/Group2/SubGroup1\" (1 members)>), ('SubGroup2', <HDF5 group \"/Group2/SubGroup2\" (1 members)>)]\n",
      "Shape of dataset3: (1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(repository_path + 'results/test_groups.h5','r') as hdf:\n",
    "    base_items = list(hdf.items())\n",
    "    print('Items in the base directory:', base_items)\n",
    "    \n",
    "    G2 = hdf.get('Group2')\n",
    "    G2_items = list(G2.items())\n",
    "    print('Items in Group2:', G2_items)\n",
    "    G21 = G2.get('SubGroup1')\n",
    "    dataset3 = np.array(G21.get('dataset3'))\n",
    "    print('Shape of dataset3:', dataset3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hdf5 compress data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(repository_path + 'results/test_groups_compressed.h5', 'w') as hdf:\n",
    "    G1 = hdf.create_group('Group1')\n",
    "    G1.create_dataset('dataset1', data = matrix1, compression='gzip', compression_opts=9)\n",
    "    G1.create_dataset('dataset4', data = matrix4, compression='gzip', compression_opts=9)\n",
    "    \n",
    "    G21 = hdf.create_group('Group2/SubGroup1')\n",
    "    G21.create_dataset('dataset3', data = matrix3, compression='gzip', compression_opts=9)\n",
    "    \n",
    "    G22 = hdf.create_group('Group2/SubGroup2')\n",
    "    G22.create_dataset('dataset2', data = matrix2, compression='gzip', compression_opts=9)\n",
    "    \n",
    "# compression from 31 to 29 M, not much to compress here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base items:  [('Group1', <HDF5 group \"/Group1\" (2 members)>), ('Group2', <HDF5 group \"/Group2\" (2 members)>)]\n",
      "['dataset1', 'dataset4']\n"
     ]
    }
   ],
   "source": [
    "# compressed data can be accessed in the same way as non-compressed data\n",
    "with h5py.File(repository_path + 'results/test_groups_compressed.h5', 'r') as hdf:\n",
    "    base_items = list(hdf.items())\n",
    "    print('Base items: ', base_items)\n",
    "    G1 = hdf.get('Group1')\n",
    "    print(list(G1.keys()))\n",
    "    data1 = G1.get('dataset1')\n",
    "    dataset1 = np.array(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(dataset1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hdf5 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.random.random(size=(1000,1000))\n",
    "matrix2 = np.random.random(size=(10000,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hdf5 file\n",
    "hdf = h5py.File(repository_path + 'results/test_attributes.h5', 'w')\n",
    "\n",
    "# create the datasets\n",
    "dataset1 = hdf.create_dataset('dataset1', data = matrix1)\n",
    "dataset2 = hdf.create_dataset('dataset2', data = matrix2)\n",
    "\n",
    "# set attributes\n",
    "dataset1.attrs['CLASS'] = 'DATA MATRIX'\n",
    "dataset1.attrs['VERSION'] = [1,2]\n",
    "\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of datasets in this file: \n",
      " ['dataset1', 'dataset2']\n",
      "Shape of dataset1: \n",
      " (1000, 1000)\n",
      "CLASS\n",
      "DATA MATRIX\n",
      "DATA MATRIX\n",
      "[1 2]\n",
      "<class 'h5py._hl.dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# read the hdf5 file\n",
    "hdf = h5py.File(repository_path + 'results/test_attributes.h5', 'r')\n",
    "ls = list(hdf.keys())\n",
    "print('List of datasets in this file: \\n', ls)\n",
    "data = hdf.get('dataset1')\n",
    "dataset1 = np.array(data)\n",
    "print('Shape of dataset1: \\n', dataset1.shape)\n",
    "# read the attributes\n",
    "k = list(data.attrs.keys())\n",
    "v = list(data.attrs.values())\n",
    "print(k[0])\n",
    "print(v[0])\n",
    "print(data.attrs[k[0]])\n",
    "print(data.attrs[k[1]])\n",
    "print(type(data))\n",
    "\n",
    "hdf.close()\n",
    "# data (which is the h5py dataset with type class 'h5py._hl.dataset.Dataset') \n",
    "# can only be accessed while hdf5 file is open for reading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create hdf5 files with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates (or opens in append mode) an hdf5 file\n",
    "hdf = pd.HDFStore(repository_path + 'results/hdf5_pandas.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {'x param': [\"A\", \"B\", \"C\", \"D\"],\n",
    "        'y param': [\"off\", \"on\", \"off\", \"off\"],\n",
    "        'z param': [1,2,3,4]}\n",
    "df1 = pd.DataFrame(data1, columns = ['x param', 'y param', 'z param'])\n",
    "hdf.put('DF1', df1, format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'city': ['Tripoli', 'Sydney', 'Tripoli', 'Rome', 'Rome', 'Tripoli', 'Rome', 'Sydney',\n",
    "                'Sydney'],\n",
    "        'rank': ['1st', '2nd', '1st', '2nd', '1st', '2nd', '1st', '2nd', '1st'],\n",
    "        'score1': [44, 48, 39, 41, 38, 44, 34, 54, 61],\n",
    "        'score2': [67, 63, 55, 70, 64, 77, 45, 66, 72]\n",
    "       }\n",
    "df2 = pd.DataFrame(data, columns = ['city', 'rank', 'score1', 'score2'])\n",
    "\n",
    "hdf.put('DF2Key', df2, format='table', data_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read hdf5 file with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open hdf5 file for reading\n",
    "hdf = pd.HDFStore(repository_path + 'results/hdf5_pandas.h5', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/DF1', '/DF2Key']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = hdf.get('/DF1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x param</th>\n",
       "      <th>y param</th>\n",
       "      <th>z param</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>off</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>on</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>off</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D</td>\n",
       "      <td>off</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  x param y param  z param\n",
       "0       A     off        1\n",
       "1       B      on        2\n",
       "2       C     off        3\n",
       "3       D     off        4"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
